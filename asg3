import numpy as np

x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])

X = np.vstack([np.ones(len(x)), x]).T
beta_hat_normal_eq = np.linalg.inv(X.T @ X) @ X.T @ y
y_hat_normal_eq = X @ beta_hat_normal_eq
SSE_normal_eq = np.sum((y - y_hat_normal_eq)**2)
R_squared_normal_eq = 1 - SSE_normal_eq / np.sum((y - np.mean(y))**2)

print("Analytic Solution:")
print(f"Coefficients: {beta_hat_normal_eq}")
print(f"Sum of Squared Errors: {SSE_normal_eq}")
print(f"R^2 value: {R_squared_normal_eq}") 
def full_batch_gradient_descent(x, y, lr=0.01, epochs=1000):
    m, b = 0.0, 0.0
    N = len(y)
    for _ in range(epochs):
        y_pred = m*x + b
        dm = (-2/N) * np.sum(x * (y - y_pred))
        db = (-2/N) * np.sum(y - y_pred)
        m -= lr * dm
        b -= lr * db
    return m, b
m_gd, b_gd = full_batch_gradient_descent(x, y)
y_pred_gd = m_gd*x + b_gd
sse_gd = np.sum((y - y_pred_gd)**2)
r_squared_gd = 1 - sse_gd / np.sum((y - np.mean(y))**2)

print("\nFull-batch Gradient Descent:")
print(f"Coefficients: {m_gd, b_gd}")
print(f"Sum of Squared Errors: {sse_gd}")
print(f"R^2 value: {r_squared_gd}")
def stochastic_gradient_descent(x, y, lr=0.01, epochs=1000):
    m, b = 0.0, 0.0
    N = len(y)
    for _ in range(epochs):
        for i in range(N):
            y_pred = m*x[i] + b
            dm = -2 * x[i] * (y[i] - y_pred)
            db = -2 * (y[i] - y_pred)
            m -= lr * dm
            b -= lr * db
    return m, b
m_sgd, b_sgd = stochastic_gradient_descent(x, y)
y_pred_sgd = m_sgd*x + b_sgd
sse_sgd = np.sum((y - y_pred_sgd)**2)
r_squared_sgd = 1 - sse_sgd / np.sum((y - np.mean(y))**2)

print("\nStochastic Gradient Descent:")
print(f"Coefficients: {m_sgd, b_sgd}")
print(f"Sum of Squared Errors: {sse_sgd}")
print(f"R^2 value: {r_squared_sgd}")
